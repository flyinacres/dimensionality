{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":396802,"sourceType":"datasetVersion","datasetId":175990}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://azad-wolf.medium.com/latent-space-representation-a-hands-on-tutorial-on-autoencoders-in-tensorflow-57735a1c0f3f","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:04:25.188668Z","iopub.execute_input":"2024-05-18T18:04:25.189223Z","iopub.status.idle":"2024-05-18T18:04:40.798195Z","shell.execute_reply.started":"2024-05-18T18:04:25.189194Z","shell.execute_reply":"2024-05-18T18:04:40.796905Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.60.0)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow)\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.1.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\nDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: keras\n  Attempting uninstall: keras\n    Found existing installation: keras 3.2.1\n    Uninstalling keras-3.2.1:\n      Successfully uninstalled keras-3.2.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport pathlib\n\n\n## create train and validation datasets\nDB_PATH = \"//kaggle/input/fashion-product-images-small/images/\"\nBUFFER_SIZE = 10000\nBATCH_SIZE = 1000\nIMG_WIDTH = 60\nIMG_HEIGHT = 60\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:11:19.627435Z","iopub.execute_input":"2024-05-18T18:11:19.627869Z","iopub.status.idle":"2024-05-18T18:11:19.635432Z","shell.execute_reply.started":"2024-05-18T18:11:19.627834Z","shell.execute_reply":"2024-05-18T18:11:19.634177Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"!pwd\n\n!ls -U //kaggle/input/fashion-product-images-small/images | head -4\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-18T18:11:01.111970Z","iopub.execute_input":"2024-05-18T18:11:01.112356Z","iopub.status.idle":"2024-05-18T18:11:01.753926Z","shell.execute_reply.started":"2024-05-18T18:11:01.112309Z","shell.execute_reply":"2024-05-18T18:11:01.752856Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"/kaggle/working\n31973.jpg\n30778.jpg\n19812.jpg\n22735.jpg\nls: write error: Broken pipe\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef load(image_file):\n    image = tf.io.read_file(image_file)\n    image = tf.image.decode_jpeg(image,channels=3)\n\n    input_image = tf.cast(image, tf.float32)\n    return input_image\n\ndef random_crop(input_image):\n    cropped_image = tf.image.random_crop(\n      input_image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n\n    return cropped_image\n\ndef resize(input_image):\n    input_image = tf.image.resize(input_image, [IMG_HEIGHT, IMG_WIDTH],\n                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    return input_image\n\ndef normalize(input_image):\n    input_image = (input_image / 255)\n    return input_image\n\n@tf.function()\ndef random_jitter(input_image):\n    input_image = random_crop(input_image)\n    if tf.random.uniform(()) > 0.5:\n        input_image = tf.image.flip_left_right(input_image)\n        \n    return input_image\n\ndef load_image_train(image_file):\n    input_image = load(image_file)\n    #input_image = random_jitter(input_image)\n    input_image = resize(input_image)\n    input_image = normalize(input_image)\n\n    return input_image,input_image\n\ndef load_image_test(image_file):\n    input_image = load(image_file)\n    #input_image = random_jitter(input_image)\n    input_image = resize(input_image)\n    input_image = normalize(input_image)\n    return input_image,input_image\n\n\ndata_dir = pathlib.Path(DB_PATH)\nimage_count = len(list(data_dir.glob('*.jpg')))\nprint(f\"image count {image_count}\")\ndataset = tf.data.Dataset.list_files(DB_PATH)\n\n\nval_size = int(image_count * 0.2)\ntrain_ds = dataset.skip(val_size)\nval_ds = dataset.take(val_size)\n\nprint(tf.data.experimental.cardinality(train_ds).numpy())\nprint(tf.data.experimental.cardinality(val_ds).numpy())\n\n\ntrain_ds = train_ds.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\ntrain_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n\nval_ds = val_ds.map(load_image_test)\nval_ds = val_ds.batch(BATCH_SIZE)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:11:24.040570Z","iopub.execute_input":"2024-05-18T18:11:24.040948Z","iopub.status.idle":"2024-05-18T18:11:24.367451Z","shell.execute_reply.started":"2024-05-18T18:11:24.040918Z","shell.execute_reply":"2024-05-18T18:11:24.365862Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"image count 44441\n0\n1\n","output_type":"stream"}]},{"cell_type":"code","source":"file_pattern = DB_PATH + \"*.jpg\"\nprint(file_pattern)\ndataset = tf.data.Dataset.list_files(file_pattern)\n\nval_size = int(image_count * 0.2)\ntrain_ds = dataset.skip(val_size)\nval_ds = dataset.take(val_size)\n\ntrain_ds.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\ntrain_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:11:27.767205Z","iopub.execute_input":"2024-05-18T18:11:27.767548Z","iopub.status.idle":"2024-05-18T18:11:34.421538Z","shell.execute_reply.started":"2024-05-18T18:11:27.767527Z","shell.execute_reply":"2024-05-18T18:11:34.420546Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"//kaggle/input/fashion-product-images-small/images/*.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"tf.data.Dataset.list_files(DB_PATH+\"*.jpg\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:04:59.802395Z","iopub.execute_input":"2024-05-18T18:04:59.802701Z","iopub.status.idle":"2024-05-18T18:05:33.317755Z","shell.execute_reply.started":"2024-05-18T18:04:59.802675Z","shell.execute_reply":"2024-05-18T18:05:33.316920Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<_ShuffleDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"},"metadata":{}}]},{"cell_type":"code","source":"tds=val_ds.take(1)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:18:51.100044Z","iopub.execute_input":"2024-05-18T18:18:51.100429Z","iopub.status.idle":"2024-05-18T18:18:51.106278Z","shell.execute_reply.started":"2024-05-18T18:18:51.100400Z","shell.execute_reply":"2024-05-18T18:18:51.105168Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"??tds","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:18:53.485924Z","iopub.execute_input":"2024-05-18T18:18:53.486281Z","iopub.status.idle":"2024-05-18T18:18:53.498153Z","shell.execute_reply.started":"2024-05-18T18:18:53.486255Z","shell.execute_reply":"2024-05-18T18:18:53.496986Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[0;31mType:\u001b[0m           _TakeDataset\n\u001b[0;31mString form:\u001b[0m    <_TakeDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n\u001b[0;31mLength:\u001b[0m         1\n\u001b[0;31mFile:\u001b[0m           /opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/take_op.py\n\u001b[0;31mSource:\u001b[0m        \n\u001b[0;32mclass\u001b[0m \u001b[0m_TakeDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnaryUnchangedStructureDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m  \u001b[0;34m\"\"\"A `Dataset` containing the first `count` elements from its input.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"See `Dataset.take()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mvariant_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_common_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mInit docstring:\u001b[0m See `Dataset.take()` for details."},"metadata":{}}]},{"cell_type":"code","source":"images, targets = val_ds.take(1)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:18:11.106413Z","iopub.execute_input":"2024-05-18T18:18:11.106818Z","iopub.status.idle":"2024-05-18T18:18:11.181368Z","shell.execute_reply.started":"2024-05-18T18:18:11.106787Z","shell.execute_reply":"2024-05-18T18:18:11.179756Z"},"trusted":true},"execution_count":22,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (images, targets) \u001b[38;5;241m=\u001b[39m val_ds\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"],"ename":"ValueError","evalue":"not enough values to unpack (expected 2, got 1)","output_type":"error"}]},{"cell_type":"code","source":"list(val_ds.as_numpy_iterator())[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:12:19.641501Z","iopub.execute_input":"2024-05-18T18:12:19.641892Z","iopub.status.idle":"2024-05-18T18:12:20.729554Z","shell.execute_reply.started":"2024-05-18T18:12:19.641862Z","shell.execute_reply":"2024-05-18T18:12:20.728388Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"b'//kaggle/input/fashion-product-images-small/images/39016.jpg'"},"metadata":{}}]},{"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nfor images,targets in val_ds.take(1):\n    for i in range(45):\n      ax = plt.subplot(3, 15, i + 1)\n      plt.imshow(images[i].numpy().astype(\"float32\"))\n      plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:11:34.640694Z","iopub.execute_input":"2024-05-18T18:11:34.641311Z","iopub.status.idle":"2024-05-18T18:11:34.725742Z","shell.execute_reply.started":"2024-05-18T18:11:34.641285Z","shell.execute_reply":"2024-05-18T18:11:34.723628Z"},"trusted":true},"execution_count":17,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images,targets \u001b[38;5;129;01min\u001b[39;00m val_ds\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m45\u001b[39m):\n\u001b[1;32m      4\u001b[0m       ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m15\u001b[39m, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/tensor.py:326\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m    324\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disallow_iteration()\n\u001b[0;32m--> 326\u001b[0m first_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_first_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _TensorIterator(\u001b[38;5;28mself\u001b[39m, first_dim)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/tensor.py:334\u001b[0m, in \u001b[0;36mTensor._get_first_dim\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot iterate over a tensor with unknown shape.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shape:\n\u001b[0;32m--> 334\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot iterate over a scalar tensor.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot iterate over a tensor with unknown first dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mTypeError\u001b[0m: Cannot iterate over a scalar tensor."],"ename":"TypeError","evalue":"Cannot iterate over a scalar tensor.","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x500 with 0 Axes>"},"metadata":{}}]}]}