{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":396802,"sourceType":"datasetVersion","datasetId":175990}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://azad-wolf.medium.com/latent-space-representation-a-hands-on-tutorial-on-autoencoders-in-tensorflow-57735a1c0f3f","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:04:25.188668Z","iopub.execute_input":"2024-05-18T18:04:25.189223Z","iopub.status.idle":"2024-05-18T18:04:40.798195Z","shell.execute_reply.started":"2024-05-18T18:04:25.189194Z","shell.execute_reply":"2024-05-18T18:04:40.796905Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.60.0)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow)\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.1.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\nDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: keras\n  Attempting uninstall: keras\n    Found existing installation: keras 3.2.1\n    Uninstalling keras-3.2.1:\n      Successfully uninstalled keras-3.2.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport pathlib\n\n\n## create train and validation datasets\nDB_PATH = \"//kaggle/input/fashion-product-images-small/images/\"\nBUFFER_SIZE = 10000\nBATCH_SIZE = 1000\nIMG_WIDTH = 60\nIMG_HEIGHT = 60\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:11:19.627435Z","iopub.execute_input":"2024-05-18T18:11:19.627869Z","iopub.status.idle":"2024-05-18T18:11:19.635432Z","shell.execute_reply.started":"2024-05-18T18:11:19.627834Z","shell.execute_reply":"2024-05-18T18:11:19.634177Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"!pwd\n\n!ls -U //kaggle/input/fashion-product-images-small/images | head -4\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-18T18:11:01.111970Z","iopub.execute_input":"2024-05-18T18:11:01.112356Z","iopub.status.idle":"2024-05-18T18:11:01.753926Z","shell.execute_reply.started":"2024-05-18T18:11:01.112309Z","shell.execute_reply":"2024-05-18T18:11:01.752856Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"/kaggle/working\n31973.jpg\n30778.jpg\n19812.jpg\n22735.jpg\nls: write error: Broken pipe\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef load(image_file):\n    image = tf.io.read_file(image_file)\n    image = tf.image.decode_jpeg(image,channels=3)\n\n    input_image = tf.cast(image, tf.float32)\n    return input_image\n\ndef random_crop(input_image):\n    cropped_image = tf.image.random_crop(\n      input_image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n\n    return cropped_image\n\ndef resize(input_image):\n    input_image = tf.image.resize(input_image, [IMG_HEIGHT, IMG_WIDTH],\n                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    return input_image\n\ndef normalize(input_image):\n    input_image = (input_image / 255)\n    return input_image\n\n@tf.function()\ndef random_jitter(input_image):\n    input_image = random_crop(input_image)\n    if tf.random.uniform(()) > 0.5:\n        input_image = tf.image.flip_left_right(input_image)\n        \n    return input_image\n\ndef load_image_train(image_file):\n    input_image = load(image_file)\n    #input_image = random_jitter(input_image)\n    input_image = resize(input_image)\n    input_image = normalize(input_image)\n\n    return input_image,input_image\n\ndef load_image_test(image_file):\n    input_image = load(image_file)\n    #input_image = random_jitter(input_image)\n    input_image = resize(input_image)\n    input_image = normalize(input_image)\n    return input_image,input_image\n\n\ndata_dir = pathlib.Path(DB_PATH)\nimage_count = len(list(data_dir.glob('*.jpg')))\nprint(f\"image count {image_count}\")\ndataset = tf.data.Dataset.list_files(DB_PATH)\n\n\nval_size = int(image_count * 0.2)\ntrain_ds = dataset.skip(val_size)\nval_ds = dataset.take(val_size)\n\nprint(tf.data.experimental.cardinality(train_ds).numpy())\nprint(tf.data.experimental.cardinality(val_ds).numpy())\n\n\ntrain_ds = train_ds.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\ntrain_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n\nval_ds = val_ds.map(load_image_test)\nval_ds = val_ds.batch(BATCH_SIZE)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:11:24.040570Z","iopub.execute_input":"2024-05-18T18:11:24.040948Z","iopub.status.idle":"2024-05-18T18:11:24.367451Z","shell.execute_reply.started":"2024-05-18T18:11:24.040918Z","shell.execute_reply":"2024-05-18T18:11:24.365862Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"image count 44441\n0\n1\n","output_type":"stream"}]},{"cell_type":"code","source":"file_pattern = DB_PATH + \"*.jpg\"\nprint(file_pattern)\ndataset = tf.data.Dataset.list_files(file_pattern)\n\nval_size = int(image_count * 0.2)\ntrain_ds = dataset.skip(val_size)\nval_ds = dataset.take(val_size)\n\ntrain_ds.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\ntrain_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:11:27.767205Z","iopub.execute_input":"2024-05-18T18:11:27.767548Z","iopub.status.idle":"2024-05-18T18:11:34.421538Z","shell.execute_reply.started":"2024-05-18T18:11:27.767527Z","shell.execute_reply":"2024-05-18T18:11:34.420546Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"//kaggle/input/fashion-product-images-small/images/*.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"tf.data.Dataset.list_files(DB_PATH+\"*.jpg\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:04:59.802395Z","iopub.execute_input":"2024-05-18T18:04:59.802701Z","iopub.status.idle":"2024-05-18T18:05:33.317755Z","shell.execute_reply.started":"2024-05-18T18:04:59.802675Z","shell.execute_reply":"2024-05-18T18:05:33.316920Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<_ShuffleDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"},"metadata":{}}]},{"cell_type":"code","source":"tds=val_ds.take(1)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:18:51.100044Z","iopub.execute_input":"2024-05-18T18:18:51.100429Z","iopub.status.idle":"2024-05-18T18:18:51.106278Z","shell.execute_reply.started":"2024-05-18T18:18:51.100400Z","shell.execute_reply":"2024-05-18T18:18:51.105168Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"??tds","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:18:53.485924Z","iopub.execute_input":"2024-05-18T18:18:53.486281Z","iopub.status.idle":"2024-05-18T18:18:53.498153Z","shell.execute_reply.started":"2024-05-18T18:18:53.486255Z","shell.execute_reply":"2024-05-18T18:18:53.496986Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[0;31mType:\u001b[0m           _TakeDataset\n\u001b[0;31mString form:\u001b[0m    <_TakeDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n\u001b[0;31mLength:\u001b[0m         1\n\u001b[0;31mFile:\u001b[0m           /opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/take_op.py\n\u001b[0;31mSource:\u001b[0m        \n\u001b[0;32mclass\u001b[0m \u001b[0m_TakeDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnaryUnchangedStructureDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m  \u001b[0;34m\"\"\"A `Dataset` containing the first `count` elements from its input.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"See `Dataset.take()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mvariant_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_common_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mInit docstring:\u001b[0m See `Dataset.take()` for details."},"metadata":{}}]},{"cell_type":"markdown","source":"The example I am following expects two results from take, but there is only 1 (a dataset).  However, it is not obvious how I extract the actual data from the dataset.  This example shows how to step through the dataset. But I only have one item, so it is clunky to have to specify a loop to get the one piece of info I need!","metadata":{}},{"cell_type":"code","source":"for elem in tds:\n  print(elem.numpy())","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:22:36.517235Z","iopub.execute_input":"2024-05-18T18:22:36.517684Z","iopub.status.idle":"2024-05-18T18:22:36.555202Z","shell.execute_reply.started":"2024-05-18T18:22:36.517652Z","shell.execute_reply":"2024-05-18T18:22:36.554391Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"b'//kaggle/input/fashion-product-images-small/images/16272.jpg'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Still a bit clunky, but this might work...","metadata":{}},{"cell_type":"code","source":"elem = next(iter(tds))\nprint(elem.numpy())","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:32:03.776580Z","iopub.execute_input":"2024-05-18T18:32:03.776932Z","iopub.status.idle":"2024-05-18T18:32:03.809710Z","shell.execute_reply.started":"2024-05-18T18:32:03.776904Z","shell.execute_reply":"2024-05-18T18:32:03.808443Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"b'//kaggle/input/fashion-product-images-small/images/14540.jpg'\n","output_type":"stream"}]},{"cell_type":"code","source":"images = (val_ds.take(1).as_numpy_iterator())[0]\nprint(images)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:25:47.875510Z","iopub.execute_input":"2024-05-18T18:25:47.875889Z","iopub.status.idle":"2024-05-18T18:25:47.914278Z","shell.execute_reply.started":"2024-05-18T18:25:47.875858Z","shell.execute_reply":"2024-05-18T18:25:47.912858Z"},"trusted":true},"execution_count":36,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_numpy_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(images)\n","\u001b[0;31mTypeError\u001b[0m: 'NumpyIterator' object is not subscriptable"],"ename":"TypeError","evalue":"'NumpyIterator' object is not subscriptable","output_type":"error"}]},{"cell_type":"code","source":"list(val_ds.as_numpy_iterator())[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:12:19.641501Z","iopub.execute_input":"2024-05-18T18:12:19.641892Z","iopub.status.idle":"2024-05-18T18:12:20.729554Z","shell.execute_reply.started":"2024-05-18T18:12:19.641862Z","shell.execute_reply":"2024-05-18T18:12:20.728388Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"b'//kaggle/input/fashion-product-images-small/images/39016.jpg'"},"metadata":{}}]},{"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nfor tds in val_ds.take(1):\n    for i in range(45):\n      ax = plt.subplot(3, 15, i + 1)\n      elem = next(iter(tds))\n      plt.imshow(elem.numpy().astype(\"float32\"))\n      plt.axis(\"off\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nsamples = 1\ntds  = val_ds.take(samples)\nfor i in range(samples):\n    ax = plt.subplot(3, 15, i + 1)\n    elem = next(iter(tds))\n    plt.imshow(elem.numpy())\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T18:36:28.019170Z","iopub.execute_input":"2024-05-18T18:36:28.019569Z","iopub.status.idle":"2024-05-18T18:36:28.937608Z","shell.execute_reply.started":"2024-05-18T18:36:28.019543Z","shell.execute_reply":"2024-05-18T18:36:28.936387Z"},"trusted":true},"execution_count":44,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[44], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m15\u001b[39m, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m elem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(tds))\n\u001b[0;32m----> 7\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/pyplot.py:2695\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2689\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[1;32m   2690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[1;32m   2691\u001b[0m         X, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2692\u001b[0m         alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, extent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2693\u001b[0m         interpolation_stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, filternorm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, filterrad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4.0\u001b[39m,\n\u001b[1;32m   2694\u001b[0m         resample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2695\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2697\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2699\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2702\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2703\u001b[0m     sci(__ret)\n\u001b[1;32m   2704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/__init__.py:1446\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1446\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1448\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1449\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1450\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axes/_axes.py:5663\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5656\u001b[0m im \u001b[38;5;241m=\u001b[39m mimage\u001b[38;5;241m.\u001b[39mAxesImage(\u001b[38;5;28mself\u001b[39m, cmap\u001b[38;5;241m=\u001b[39mcmap, norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[1;32m   5657\u001b[0m                       interpolation\u001b[38;5;241m=\u001b[39minterpolation, origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[1;32m   5658\u001b[0m                       extent\u001b[38;5;241m=\u001b[39mextent, filternorm\u001b[38;5;241m=\u001b[39mfilternorm,\n\u001b[1;32m   5659\u001b[0m                       filterrad\u001b[38;5;241m=\u001b[39mfilterrad, resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[1;32m   5660\u001b[0m                       interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5661\u001b[0m                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 5663\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5664\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5666\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/image.py:701\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39msafe_masked_invalid(A, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8 \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mcan_cast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_kind\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 701\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage data of dtype \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m cannot be converted to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    702\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# If just one dimension assume scalar and apply colormap\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A[:, :, \u001b[38;5;241m0\u001b[39m]\n","\u001b[0;31mTypeError\u001b[0m: Image data of dtype |S60 cannot be converted to float"],"ename":"TypeError","evalue":"Image data of dtype |S60 cannot be converted to float","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x500 with 1 Axes>","image/png":"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"},"metadata":{}}]}]}