{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":396802,"sourceType":"datasetVersion","datasetId":175990}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://azad-wolf.medium.com/latent-space-representation-a-hands-on-tutorial-on-autoencoders-in-tensorflow-57735a1c0f3f","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pwd\n\n!ls //kaggle/input/fashion-product-images-small/images\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport pathlib\n\n\n## create train and validation datasets\nDB_PATH = \"//kaggle//input//fashion-product-images-small//images//\"\nBUFFER_SIZE = 10000\nBATCH_SIZE = 1000\nIMG_WIDTH = 60\nIMG_HEIGHT = 60\n\ndef load(image_file):\n    image = tf.io.read_file(image_file)\n    image = tf.image.decode_jpeg(image,channels=3)\n\n    input_image = tf.cast(image, tf.float32)\n    return input_image\n\ndef random_crop(input_image):\n    cropped_image = tf.image.random_crop(\n      input_image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n\n    return cropped_image\n\ndef resize(input_image):\n    input_image = tf.image.resize(input_image, [IMG_HEIGHT, IMG_WIDTH],\n                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    return input_image\n\ndef normalize(input_image):\n    input_image = (input_image / 255)\n    return input_image\n\n@tf.function()\ndef random_jitter(input_image):\n    input_image = random_crop(input_image)\n    if tf.random.uniform(()) > 0.5:\n        input_image = tf.image.flip_left_right(input_image)\n        \n    return input_image\n\ndef load_image_train(image_file):\n    input_image = load(image_file)\n    #input_image = random_jitter(input_image)\n    input_image = resize(input_image)\n    input_image = normalize(input_image)\n\n    return input_image,input_image\n\ndef load_image_test(image_file):\n    input_image = load(image_file)\n    #input_image = random_jitter(input_image)\n    input_image = resize(input_image)\n    input_image = normalize(input_image)\n    return input_image,input_image\n\n\ndata_dir = pathlib.Path(DB_PATH)\nimage_count = len(list(data_dir.glob('*.jpg')))\ndataset = tf.data.Dataset.list_files(DB_PATH)\n\n\nval_size = int(image_count * 0.2)\ntrain_ds = dataset.skip(val_size)\nval_ds = dataset.take(val_size)\n\nprint(tf.data.experimental.cardinality(train_ds).numpy())\nprint(tf.data.experimental.cardinality(val_ds).numpy())\n\n\ntrain_ds = train_ds.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\ntrain_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n\nval_ds = val_ds.map(load_image_test)\nval_ds = val_ds.batch(BATCH_SIZE)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T17:01:32.017584Z","iopub.execute_input":"2024-05-08T17:01:32.018019Z","iopub.status.idle":"2024-05-08T17:01:32.873452Z","shell.execute_reply.started":"2024-05-08T17:01:32.017990Z","shell.execute_reply":"2024-05-08T17:01:32.872075Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"0\n1\n","output_type":"stream"}]}]}